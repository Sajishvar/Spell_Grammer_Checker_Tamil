{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sajishvar/Spell_Grammer_Checker_Tamil/blob/main/Spell_Grammer_Checker_Tamil.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SPELL CHECKER**"
      ],
      "metadata": {
        "id": "pthVUKRBq9fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from collections import Counter\n",
        "\n",
        "# Adjust encoding configuration for standard output in environments supporting it.\n",
        "if hasattr(sys.stdout, 'reconfigure'):\n",
        "    sys.stdout.reconfigure(encoding='utf-8')\n",
        "\n",
        "def load_dictionary(file_path):\n",
        "    # Load words from a dictionary file\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        dictionary = set(line.strip() for line in file)\n",
        "    return dictionary"
      ],
      "metadata": {
        "id": "CEFMekH6Lvnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HEURISTICS**"
      ],
      "metadata": {
        "id": "gDq_OrVFrQH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_length_similarity(misspelt_word, candidate_word):\n",
        "    # Binary score: 1.0 if lengths match, 0.0 if they don't\n",
        "    return 1.0 if len(misspelt_word) == len(candidate_word) else 0.0\n",
        "\n",
        "def character_frequency_similarity(misspelt_word, candidate_word):\n",
        "    # Count frequency of each character in both words\n",
        "    misspelt_counter = Counter(misspelt_word)\n",
        "    candidate_counter = Counter(candidate_word)\n",
        "    # Compute sum of minimum matches for each character\n",
        "    matching_count = sum(min(misspelt_counter[char], candidate_counter[char]) for char in misspelt_counter)\n",
        "    # Normalize by the length of the misspelt word\n",
        "    return matching_count / len(misspelt_word)\n",
        "\n",
        "def position_similarity(misspelt_word, candidate_word):\n",
        "    # Count matching characters at the same positions\n",
        "    match_count = sum(1 for m_char, c_char in zip(misspelt_word, candidate_word) if m_char == c_char)\n",
        "\n",
        "    # Normalize by the length of the shorter word to avoid penalizing due to different lengths\n",
        "    return match_count / min(len(misspelt_word), len(candidate_word))\n",
        "\n",
        "def first_letter_similarity(misspelt_word, candidate_word):\n",
        "    # Check if the first letters of both words match\n",
        "    return 1.0 if misspelt_word[0] == candidate_word[0] else 0.0\n",
        "\n",
        "def levenshtein_distance(word1, word2):\n",
        "    # Compute the Levenshtein distance between two words\n",
        "    len_word1, len_word2 = len(word1), len(word2)\n",
        "    matrix = [[0] * (len_word2 + 1) for _ in range(len_word1 + 1)]\n",
        "\n",
        "    for i in range(len_word1 + 1):\n",
        "        matrix[i][0] = i\n",
        "    for j in range(len_word2 + 1):\n",
        "        matrix[0][j] = j\n",
        "\n",
        "    for i in range(1, len_word1 + 1):\n",
        "        for j in range(1, len_word2 + 1):\n",
        "            cost = 0 if word1[i-1] == word2[j-1] else 1\n",
        "            matrix[i][j] = min(\n",
        "                matrix[i-1][j] + 1,  # Deletion\n",
        "                matrix[i][j-1] + 1,  # Insertion\n",
        "                matrix[i-1][j-1] + cost  # Substitution\n",
        "            )\n",
        "\n",
        "    return matrix[len_word1][len_word2]\n",
        "\n",
        "def distance_similarity(misspelt_word, candidate_word):\n",
        "    # Levenshtein distance normalized by the maximum possible distance\n",
        "    max_distance = max(len(misspelt_word), len(candidate_word))\n",
        "    lev_distance = levenshtein_distance(misspelt_word, candidate_word)\n",
        "    return 1 - (lev_distance / max_distance)  # Return similarity score (1 for exact match)\n",
        "\n"
      ],
      "metadata": {
        "id": "2RWXXt4jkxgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SCORES**"
      ],
      "metadata": {
        "id": "N6VMaB94rZee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_similarity_score(misspelt_word, candidate_word):\n",
        "    # Weights\n",
        "    weight_length = 0.2\n",
        "    weight_frequency = 0.2\n",
        "    weight_position = 0.2\n",
        "    weight_first_letter = 0.2\n",
        "    weight_distance = 0.2\n",
        "\n",
        "    # Heuristic scores\n",
        "    length_score = word_length_similarity(misspelt_word, candidate_word)\n",
        "    frequency_score = character_frequency_similarity(misspelt_word, candidate_word)\n",
        "    position_score = position_similarity(misspelt_word, candidate_word)\n",
        "    first_letter_score = first_letter_similarity(misspelt_word, candidate_word)\n",
        "    distance_score = distance_similarity(misspelt_word, candidate_word)\n",
        "\n",
        "    # Weighted sum of the scores\n",
        "    return (weight_length * length_score +\n",
        "            weight_frequency * frequency_score +\n",
        "            weight_position * position_score +\n",
        "            weight_first_letter * first_letter_score +\n",
        "            weight_distance * distance_score)"
      ],
      "metadata": {
        "id": "WksDCtgwq5bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CORRECTION SUGGESTION**"
      ],
      "metadata": {
        "id": "aC7yr-Mttif7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_corrections(misspelt_word, dictionary, threshold=0.5):\n",
        "    # Calculate similarity scores for each word in the dictionary\n",
        "    scored_candidates = [\n",
        "        (candidate, calculate_similarity_score(misspelt_word, candidate))\n",
        "        for candidate in dictionary\n",
        "    ]\n",
        "    # Filter candidates with scores above threshold\n",
        "    scored_candidates = [(word, score) for word, score in scored_candidates if score > threshold]\n",
        "    # Sort by score in descending order\n",
        "    scored_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return scored_candidates"
      ],
      "metadata": {
        "id": "7nTo7Rintix7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SENTENCE PROCCESSING**"
      ],
      "metadata": {
        "id": "4TIGDu5BuN4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_sentence(sentence, dictionary, threshold=0.5):\n",
        "    words = sentence.split()  # Split the sentence into words\n",
        "    corrections = {}  # Dictionary to store corrections for each misspelled word\n",
        "    corrected_sentence = []  # List to build the corrected sentence\n",
        "\n",
        "    for word in words:\n",
        "        # Get suggestions for each word\n",
        "        suggestions = suggest_corrections(word, dictionary, threshold)\n",
        "\n",
        "        if suggestions:\n",
        "            # If the top suggestion has a perfect score, the word is correct\n",
        "            if suggestions[0][1] == 1.0:\n",
        "                corrections[word] = \"Correct\"\n",
        "                corrected_sentence.append(word)  # Keep the original word\n",
        "            else:\n",
        "                # Otherwise, save the top suggestions\n",
        "                corrections[word] = [s[0] for s in suggestions[:5]]  # Top 5 suggestions\n",
        "                corrected_sentence.append(suggestions[0][0])  # Use the top suggestion\n",
        "        else:\n",
        "            corrections[word] = \"No suggestions\"  # No suitable suggestions\n",
        "            corrected_sentence.append(word)  # Keep the original word\n",
        "\n",
        "    return corrections, ' '.join(corrected_sentence)"
      ],
      "metadata": {
        "id": "HmiU6E_IuJxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**USAGE**"
      ],
      "metadata": {
        "id": "8OHjWDY2yO7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "dictionary_file = '/content/tamil_words.txt'  # Replace with your Tamil words file\n",
        "sentence = 'நான் காற்ற பார்ப்பேன்'  # Replace with your Tamil sentence\n",
        "\n",
        "# Load dictionary\n",
        "dictionary = load_dictionary(dictionary_file)\n",
        "\n",
        "# Process the sentence\n",
        "corrections, corrected_sentence = process_sentence(sentence, dictionary)\n",
        "\n",
        "# Print corrections for the sentence\n",
        "for word, suggestion in corrections.items():\n",
        "    if suggestion == \"Correct\":\n",
        "        print(f\"'{word}': Correct\")\n",
        "    elif suggestion == \"No suggestions\":\n",
        "        print(f\"'{word}': No suggestions found\")\n",
        "    else:\n",
        "        print(f\"'{word}': Suggestions: {', '.join(suggestion)}\")\n",
        "\n",
        "# Print the corrected sentence\n",
        "print(\"Corrected Sentence:\", corrected_sentence)"
      ],
      "metadata": {
        "id": "5cpebTzQyZcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRAMMAR CHECKER**"
      ],
      "metadata": {
        "id": "ntq-yoGvd3tA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STANZA PIPELINE FOR POS TAGGING**"
      ],
      "metadata": {
        "id": "6UMUMdb9eRAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stanza\n",
        "\n",
        "import stanza\n",
        "from collections import Counter\n",
        "\n",
        "# Load Tamil Stanza pipeline for POS tagging\n",
        "stanza.download('ta')  # Download the Tamil language model if not already done\n",
        "nlp = stanza.Pipeline('ta', processors='tokenize,pos', use_gpu=False)"
      ],
      "metadata": {
        "id": "kSgl10F_eEss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POS TAGGING**"
      ],
      "metadata": {
        "id": "nyPOW0WqedKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the sentence\n",
        "doc = nlp(corrected_sentence)\n",
        "\n",
        "# Print POS tags for each word\n",
        "print(\"Word\\tPOS\")\n",
        "for sent in doc.sentences:\n",
        "    for word in sent.words:\n",
        "        print(f\"{word.text}\\t{word.upos}\")"
      ],
      "metadata": {
        "id": "QAxxtcBbexCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RULE BASED GRAMMAR CHECKING**"
      ],
      "metadata": {
        "id": "pq2V3Nmdezxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_and_correct_grammar(sentence):\n",
        "    \"\"\"\n",
        "    Check the grammar of a Tamil sentence, detect multiple issues, and produce a fully corrected sentence.\n",
        "    \"\"\"\n",
        "    # Process the sentence with Stanza\n",
        "    doc = nlp(sentence)\n",
        "    errors = []\n",
        "    corrected_words = sentence.split()  # Start with the original words\n",
        "\n",
        "    # Extract words and POS tags\n",
        "    words = []\n",
        "    pos_tags = []\n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            words.append(word.text)\n",
        "            pos_tags.append(word.upos)\n",
        "\n",
        "    # Rule 1: Subject-Object-Verb Order\n",
        "    if 'PRON' in pos_tags and 'NOUN' in pos_tags and 'VERB' in pos_tags:\n",
        "        pron_index = pos_tags.index('PRON')\n",
        "        noun_index = pos_tags.index('NOUN')\n",
        "        verb_index = pos_tags.index('VERB')\n",
        "        if not (pron_index < noun_index < verb_index):\n",
        "            errors.append(\"Error: The sentence should follow Subject-Object-Verb (SOV) order.\")\n",
        "            # Correct the word order\n",
        "            corrected_order = [words[pron_index], words[noun_index], words[verb_index]]\n",
        "            corrected_words = corrected_order + words[verb_index + 1:]\n",
        "\n",
        "    # Rule 2: Adjective-Noun Order\n",
        "    if 'ADJ' in pos_tags and 'NOUN' in pos_tags:\n",
        "        adj_index = pos_tags.index('ADJ')\n",
        "        noun_index = pos_tags.index('NOUN')\n",
        "        if adj_index > noun_index:\n",
        "            errors.append(\"Error: Adjectives should precede the noun they modify.\")\n",
        "            # Correct adjective-noun order\n",
        "            corrected_words[adj_index], corrected_words[noun_index] = corrected_words[noun_index], corrected_words[adj_index]\n",
        "\n",
        "    # Rule 4: Plural Agreement\n",
        "    if 'PRON' in pos_tags and 'VERB' in pos_tags:\n",
        "        pron_word = words[pos_tags.index('PRON')]\n",
        "        verb_word = words[pos_tags.index('VERB')]\n",
        "        if pron_word.endswith(\"ள்\") and not verb_word.endswith(\"ோம்\"):\n",
        "            errors.append(\"Error: Plural pronoun should match plural verb form.\")\n",
        "            # Fix plural verb agreement\n",
        "            corrected_words[pos_tags.index('VERB')] = verb_word.replace(\"ேன்\", \"ோம்\")\n",
        "\n",
        "    # Return errors and corrections\n",
        "    if errors:\n",
        "        corrected_sentence = \" \".join(corrected_words)\n",
        "        return {\n",
        "            \"status\": \"errors\",\n",
        "            \"details\": errors,\n",
        "            \"corrected_sentence\": corrected_sentence,\n",
        "        }\n",
        "    else:\n",
        "        return {\"status\": \"correct\", \"details\": \"The sentence is grammatically correct.\"}\n"
      ],
      "metadata": {
        "id": "ZzI3xapJe8xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXAMPLE USAGE**"
      ],
      "metadata": {
        "id": "rafSN7irfAi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"பள்ளிக்கு நாங்கள் செல்வேன்\"  # Incorrect Tamil sentence\n",
        "result = check_and_correct_grammar(sentence)\n",
        "\n",
        "# Display Results\n",
        "if result[\"status\"] == \"correct\":\n",
        "    print(result[\"details\"])\n",
        "else:\n",
        "    print(\"Grammar Errors Found:\")\n",
        "    for error in result[\"details\"]:\n",
        "        print(f\"- {error}\")\n",
        "    if \"corrected_sentence\" in result:\n",
        "        print(f\"Corrected Sentence: {result['corrected_sentence']}\")"
      ],
      "metadata": {
        "id": "7qB6WthGfDPU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}